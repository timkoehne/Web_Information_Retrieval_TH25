{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval Lab WiSe 2024/2025: Baseline Retrieval System\n",
    "\n",
    "This Jupyter notebook serves as a baseline retrieval system that you can improve upon.\n",
    "We use subsets of the MS MARCO datasets to retrieve passages of web documents.\n",
    "We will show you how to create a software submission to TIRA from this notebook.\n",
    "\n",
    "An overview of all corpora that we use in the current course is available at [https://tira.io/datasets?query=ir-lab-wise-2024](https://tira.io/datasets?query=ir-lab-wise-2024). The dataset IDs for loading the datasets are:\n",
    "\n",
    "- `ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training`: A subsample of the TREC 2019/2020 Deep Learning tracks on the MS MARCO v1 passage dataset. Use this dataset to tune your system(s).\n",
    "- `ir-lab-wise-2024/subsampled-ms-marco-rag-20241202-training` (_work in progress_): A subsample of the TREC 2024 Retrieval-Augmented Generation track on the MS MARCO v2.1 passage dataset. Use this dataset to tune your system(s).\n",
    "- `ir-lab-wise-2024/ms-marco-rag-20241203-test` (work in progress): The test corpus that we have created together in the course, based on the MS MARCO v2.1 passage dataset. We will use this dataset as the test dataset, i.e., evaluation scores become available only after the submission deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import libraries\n",
    "\n",
    "We will use [tira](https://tira.io/), an information retrieval shared task platform, and [ir_dataset](https://ir-datasets.com/) for loading the datasets. Subsequently, we will build a retrieval system with [PyTerrier](https://github.com/terrier-org/pyterrier), an open-source search engine framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to install the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install \"tira>=0.0.139\" ir-datasets \"python-terrier==0.10.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an API client to interact with the TIRA platform (e.g., to load datasets and submit runs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded\n",
    "from tira.rest_api_client import Client\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the dataset\n",
    "\n",
    "We load the dataset by its ir_datasets ID (as listed in the Readme). Just be sure to add the `irds:` prefix before the dataset ID to tell PyTerrier to load the data from ir_datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-wise-2024/subsampled-ms-marco-deep-learning-20241201-training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build an index\n",
    "\n",
    "We will then create an index from the documents in the dataset we just loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "indexer = pt.IterDictIndexer(\n",
    "    index_path=os.getcwd() + os.sep + \"index\",\n",
    "    meta={'docno': 50, 'text': 4096},\n",
    "    # If an index already exists there, then overwrite it.\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "index = indexer.index(pt_dataset.get_corpus_iter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define the retrieval pipeline\n",
    "\n",
    "We will define a simple retrieval pipeline using just BM25 as a baseline. For details, refer to the PyTerrier [documentation](https://pyterrier.readthedocs.io) or [tutorial](https://github.com/terrier-org/ecir2021tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25 = pt.BatchRetrieve(index, wmodel=\"BM25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using WordNet and synset_similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use nltk stopwords and lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Literal, Set, Tuple\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create some Helper-Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_synset_similarity(word1, word2):\n",
    "    try:\n",
    "        synset1 = wordnet.synsets(word1)[0]\n",
    "        synset2 = wordnet.synsets(word2)[0]\n",
    "        return synset1.path_similarity(synset2)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def remove_bad_characters(text):\n",
    "    text = text.replace(\"'s\", \"\")\n",
    "    text = re.sub(r'[^a-zA-Z0-9_]', '', text)\n",
    "    return text\n",
    "\n",
    "def filter_min_similarity(synonyms: Set[Tuple[str, float]], similarity: float) -> Set[Tuple[str, float]]:\n",
    "    synonyms = set(filter(lambda x: x[1] > similarity, synonyms))\n",
    "    return synonyms\n",
    "\n",
    "def find_top_k(synonyms: Set[Tuple[str, float]], k: int) -> Set[Tuple[str, float]]:\n",
    "    sorted_synonyms_list = sorted(synonyms, key=lambda x: x[1], reverse=True)\n",
    "    synonyms = set(sorted_synonyms_list[:min(k, len(synonyms))])\n",
    "    return synonyms\n",
    "\n",
    "def add_synset_similarity(synonyms_in: Set[str], term: str) -> Set[Tuple[str, float]]:\n",
    "    return set([(name, get_synset_similarity(term, name)) for name in synonyms_in])\n",
    "\n",
    "def remove_stopwords(query):\n",
    "    filtered_query = [w for w in query.split() if w not in stop_words]\n",
    "    return \" \".join(filtered_query)\n",
    "\n",
    "def lemmatize_query(query):\n",
    "    lemmatized_query = []\n",
    "    for word in query.split():\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        lemmatized_query.append(lemma)\n",
    "    return \" \".join(lemmatized_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pyterrier Transformer to modify the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordnetQueryModifier(pt.Transformer):\n",
    "    def __init__(self, min_similarity: float, top_k: int, pos: Literal[\"noun\"] | Literal[\"verb\"] | Literal[\"adjective\"] | None = None):\n",
    "        self.min_similarity = min_similarity\n",
    "        self.top_k = top_k\n",
    "        self.pos = pos[0] if pos != None else None\n",
    "        \n",
    "    def transform(self, queries: pd.DataFrame):\n",
    "        queries[\"query\"] = queries[\"query\"].apply(self.expand_query_wordnet)\n",
    "        return queries\n",
    "    \n",
    "    def expand_query_wordnet(self, query):\n",
    "        query = remove_stopwords(query)\n",
    "        query = lemmatize_query(query)\n",
    "        expanded_query = query.split()\n",
    "        \n",
    "        # find synonyms\n",
    "        for term in query.split():\n",
    "            synonyms: Set[str] = set()\n",
    "            for syn in wordnet.synsets(term, self.pos):\n",
    "                for lemma in syn.lemmas():\n",
    "                    name = lemma.name()\n",
    "                    name = remove_bad_characters(name)\n",
    "                    synonyms.add(name)\n",
    "                    \n",
    "            # only select some synonyms\n",
    "            synonyms_with_similarity: Set[Tuple[str, float]] = add_synset_similarity(synonyms, term)\n",
    "            synonyms_with_similarity_filtered: Set[Tuple[str, float]] = filter_min_similarity(synonyms_with_similarity, self.min_similarity)\n",
    "            synonyms_with_similarity_filtered_top_k: Set[Tuple[str, float]] = find_top_k(synonyms_with_similarity_filtered, self.top_k)\n",
    "            \n",
    "            # add selected synonyms to query\n",
    "            synonyms_words: list[str] = [syn[0] for syn in synonyms_with_similarity_filtered_top_k]\n",
    "            for synonym in synonyms_words:\n",
    "                if len(expanded_query) < 64:\n",
    "                    if synonym not in expanded_query and synonym != term and synonym.lower() != term.lower():\n",
    "                        expanded_query.append(synonym)\n",
    "                else:\n",
    "                    print(f\"query '{query}' has too many synonyms to add: {synonyms_words}\")\n",
    "        \n",
    "        # print(f' final query \\\"{\" \".join(expanded_query)}\\\"')\n",
    "        return \" \".join(expanded_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_modifier = WordnetQueryModifier(0.5, 3, None)\n",
    "wordnet_modifier.transform(pt_dataset.get_topics('text')[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate your run\n",
    " This uses ParameterGrid from sklearn.model_selection instead of pt.GridScan since GridScan uses the modified queries from a previous attempt in the next one\n",
    " Gridsearch find the best parameters for:\n",
    " - minimum synset_similarity\n",
    " - top_k value\n",
    " - word_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "import tqdm\n",
    "\n",
    "params = {\n",
    "    \"min_similarity\": [0.1*x for x in range(0, 11)],\n",
    "    \"top_k\": [x for x in range(1, 6)],\n",
    "    \"pos\": [\"noun\", \"verb\", \"adjective\", None]\n",
    "}\n",
    "param_grid = ParameterGrid(params)\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for p in tqdm.tqdm(param_grid):\n",
    "    wordnet_query_modifier = WordnetQueryModifier(**p)\n",
    "    wordnet_pipeline = wordnet_query_modifier >> bm25\n",
    "\n",
    "    experiment_results = pt.Experiment(\n",
    "        [wordnet_pipeline],\n",
    "        pt_dataset.get_topics('text'),\n",
    "        pt_dataset.get_qrels(),\n",
    "        eval_metrics = [\"map\", \"recip_rank\", \"ndcg_cut_10\", \"P_1\", \"P_5\", \"P_10\"]\n",
    "    )\n",
    "    results.append(experiment_results)\n",
    "    names.append(str(p))\n",
    "\n",
    "all_results = pd.concat(results, keys=names)\n",
    "all_results = all_results.reset_index().drop([\"name\", \"level_1\"], axis=1).rename(columns={\"level_0\": \"name\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>map</th>\n",
       "      <th>recip_rank</th>\n",
       "      <th>ndcg_cut_10</th>\n",
       "      <th>P_1</th>\n",
       "      <th>P_5</th>\n",
       "      <th>P_10</th>\n",
       "      <th>row_average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>{'min_similarity': 1.0, 'pos': None, 'top_k': 3}</td>\n",
       "      <td>0.417705</td>\n",
       "      <td>0.794299</td>\n",
       "      <td>0.491219</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>0.620619</td>\n",
       "      <td>0.575258</td>\n",
       "      <td>0.601740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>{'min_similarity': 1.0, 'pos': None, 'top_k': 5}</td>\n",
       "      <td>0.417705</td>\n",
       "      <td>0.794299</td>\n",
       "      <td>0.491219</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>0.620619</td>\n",
       "      <td>0.575258</td>\n",
       "      <td>0.601740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>{'min_similarity': 1.0, 'pos': None, 'top_k': 4}</td>\n",
       "      <td>0.417705</td>\n",
       "      <td>0.794299</td>\n",
       "      <td>0.491219</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>0.620619</td>\n",
       "      <td>0.575258</td>\n",
       "      <td>0.601740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>{'min_similarity': 1.0, 'pos': 'noun', 'top_k'...</td>\n",
       "      <td>0.417705</td>\n",
       "      <td>0.794299</td>\n",
       "      <td>0.491219</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>0.620619</td>\n",
       "      <td>0.575258</td>\n",
       "      <td>0.601740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>{'min_similarity': 1.0, 'pos': 'noun', 'top_k'...</td>\n",
       "      <td>0.417705</td>\n",
       "      <td>0.794299</td>\n",
       "      <td>0.491219</td>\n",
       "      <td>0.711340</td>\n",
       "      <td>0.620619</td>\n",
       "      <td>0.575258</td>\n",
       "      <td>0.601740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'min_similarity': 0.0, 'pos': None, 'top_k': 3}</td>\n",
       "      <td>0.352749</td>\n",
       "      <td>0.610806</td>\n",
       "      <td>0.378328</td>\n",
       "      <td>0.484536</td>\n",
       "      <td>0.480412</td>\n",
       "      <td>0.461856</td>\n",
       "      <td>0.461448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'min_similarity': 0.1, 'pos': None, 'top_k': 4}</td>\n",
       "      <td>0.336860</td>\n",
       "      <td>0.568524</td>\n",
       "      <td>0.351076</td>\n",
       "      <td>0.422680</td>\n",
       "      <td>0.457732</td>\n",
       "      <td>0.436082</td>\n",
       "      <td>0.428826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'min_similarity': 0.0, 'pos': None, 'top_k': 4}</td>\n",
       "      <td>0.329683</td>\n",
       "      <td>0.568963</td>\n",
       "      <td>0.345760</td>\n",
       "      <td>0.432990</td>\n",
       "      <td>0.447423</td>\n",
       "      <td>0.431959</td>\n",
       "      <td>0.426130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>{'min_similarity': 0.1, 'pos': None, 'top_k': 5}</td>\n",
       "      <td>0.330339</td>\n",
       "      <td>0.552143</td>\n",
       "      <td>0.337982</td>\n",
       "      <td>0.402062</td>\n",
       "      <td>0.443299</td>\n",
       "      <td>0.423711</td>\n",
       "      <td>0.414923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'min_similarity': 0.0, 'pos': None, 'top_k': 5}</td>\n",
       "      <td>0.319092</td>\n",
       "      <td>0.535555</td>\n",
       "      <td>0.327426</td>\n",
       "      <td>0.391753</td>\n",
       "      <td>0.424742</td>\n",
       "      <td>0.414433</td>\n",
       "      <td>0.402167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>220 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  name       map  recip_rank  \\\n",
       "217   {'min_similarity': 1.0, 'pos': None, 'top_k': 3}  0.417705    0.794299   \n",
       "219   {'min_similarity': 1.0, 'pos': None, 'top_k': 5}  0.417705    0.794299   \n",
       "218   {'min_similarity': 1.0, 'pos': None, 'top_k': 4}  0.417705    0.794299   \n",
       "202  {'min_similarity': 1.0, 'pos': 'noun', 'top_k'...  0.417705    0.794299   \n",
       "203  {'min_similarity': 1.0, 'pos': 'noun', 'top_k'...  0.417705    0.794299   \n",
       "..                                                 ...       ...         ...   \n",
       "17    {'min_similarity': 0.0, 'pos': None, 'top_k': 3}  0.352749    0.610806   \n",
       "38    {'min_similarity': 0.1, 'pos': None, 'top_k': 4}  0.336860    0.568524   \n",
       "18    {'min_similarity': 0.0, 'pos': None, 'top_k': 4}  0.329683    0.568963   \n",
       "39    {'min_similarity': 0.1, 'pos': None, 'top_k': 5}  0.330339    0.552143   \n",
       "19    {'min_similarity': 0.0, 'pos': None, 'top_k': 5}  0.319092    0.535555   \n",
       "\n",
       "     ndcg_cut_10       P_1       P_5      P_10  row_average  \n",
       "217     0.491219  0.711340  0.620619  0.575258     0.601740  \n",
       "219     0.491219  0.711340  0.620619  0.575258     0.601740  \n",
       "218     0.491219  0.711340  0.620619  0.575258     0.601740  \n",
       "202     0.491219  0.711340  0.620619  0.575258     0.601740  \n",
       "203     0.491219  0.711340  0.620619  0.575258     0.601740  \n",
       "..           ...       ...       ...       ...          ...  \n",
       "17      0.378328  0.484536  0.480412  0.461856     0.461448  \n",
       "38      0.351076  0.422680  0.457732  0.436082     0.428826  \n",
       "18      0.345760  0.432990  0.447423  0.431959     0.426130  \n",
       "39      0.337982  0.402062  0.443299  0.423711     0.414923  \n",
       "19      0.327426  0.391753  0.424742  0.414433     0.402167  \n",
       "\n",
       "[220 rows x 8 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results['row_average'] = all_results[['map', 'recip_rank', 'ndcg_cut_10', 'P_1', 'P_5', 'P_10']].mean(axis=1)\n",
    "df_sorted = all_results.sort_values(by='row_average', ascending=False)\n",
    "df_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
